{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of DeepBugs replication\n",
        "following readme on github page\n",
        "* All commands are called from the main directory.\n",
        "* Python code (most of the implementation) and JavaScript code (for extracting data from .js files) are in the `/python` and `/javascript` directories.\n",
        "* All data to learn from, e.g., .js files are expected to be in the `/data` directory.\n",
        "* All data that is generated, e.g., intermediate representations, are written into the main directory. It is recommended to move them into separate directories.\n",
        "* All generated data files have a timestamp as part of the file name. Below, all files are used with `*`. When running commands multiple times, make sure to use the most recent files."
      ],
      "metadata": {
        "id": "UVMmc4V0Tbnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone github for DeepBugs\n",
        "I forked the project to my own github in order to corrections to the code.\n",
        "Certain keras imports are depricated causing errors.\n",
        "\n",
        "* depricated import\n",
        "\n",
        "`from tensorflow.python.keras.models import Sequential`\n",
        "\n",
        "`from tensorflow.python.keras.layers.core import Dense, Dropout`\n",
        "\n",
        "\n",
        "* correct import statement\n",
        "\n",
        "`from keras.models import Sequential`\n",
        "\n",
        "`from keras.layers import Dense, Dropout`"
      ],
      "metadata": {
        "id": "rzZHBWY6IIvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEsqVSEN2-4c"
      },
      "outputs": [],
      "source": [
        "! git clone -b working https://github.com/livingdan/DeepBugs_replication"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move contents of DeepBugs to main directory\n",
        "! mv DeepBugs_replication/* ."
      ],
      "metadata": {
        "id": "4-q5fGpEOWuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  install dependencies npm modules acorn, estraverse, walk-sync\n",
        "! npm install acorn\n",
        "! npm install estraverse\n",
        "! npm install walk-sync"
      ],
      "metadata": {
        "id": "FUFFyWrYOhuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download Training and Testing datasets\n",
        "### Two options for using training/testing data\n",
        "* The full corpus can be downloaded [here](http://www.srl.inf.ethz.ch/js150.php) and is expected to be stored in `data/js/programs_all`. It consists of 100.000 training files, listed in `data/js/programs_training.txt`, and 50.000 files for validation, listed in `data/js/programs_eval.txt`.\n",
        "* This repository contains only a very small subset of the corpus. It is stored in `data/js/programs_50`. Training and validation files for the small corpus are listed in `data/js/programs_50_training.txt` and `data/js/programs_50_eval.txt`."
      ],
      "metadata": {
        "id": "5jHa7XcOIQaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown http://files.srl.inf.ethz.ch/data/js_dataset.tar.gz"
      ],
      "metadata": {
        "id": "nSxmTFwu4FUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! tar -xzf js_dataset.tar.gz"
      ],
      "metadata": {
        "id": "XPWp50YL8l7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data/js/programs_all\n",
        "! tar -xzf data.tar.gz -C data/js/programs_all\n"
      ],
      "metadata": {
        "id": "EYymYnCgFqbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mv data/js/programs_all/data/* data/js/programs_all"
      ],
      "metadata": {
        "id": "Mh2Htpi9InJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Learning a Bug Detector\n",
        "Creating a bug detector consists of two main steps:\n",
        "\n",
        "1. Extract positive (i.e., likely correct) and negative (i.e., likely buggy) training examples from code.\n",
        "\n",
        "2. Train a classifier to distinguish correct from incorrect code examples.\n",
        "\n",
        "This replication example will address the swapped argument bug detector\n",
        "* The `SwappedArgs` bug detector looks for accidentally swapped arguments of a function call, e.g., calling `setPoint(y,x)` instead of `setPoint(x,y)`."
      ],
      "metadata": {
        "id": "5ccdc3jYIjgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Extract positive and Negative training examples\n",
        "\n",
        "`node javascript/extractFromJS.js calls --parallel 4 data/js/programs_50_training.txt data/js/programs_50`\n",
        "\n",
        "  * The `--parallel` argument sets the number of processes to run.\n",
        "  * `programs_50_training.txt` contains files to include (one file per line). To extract data for validation, run the command with `data/js/programs_50_eval.txt`.\n",
        "  * The last argument is a directory that gets recursively scanned for .js files, considering only files listed in the file provided as the second argument.\n",
        "  * The command produces `calls_*.json` files, which is data suitable for the `SwappedArgs` bug detector. For the other bug two detectors, replace `calls` with `binOps` in the above command."
      ],
      "metadata": {
        "id": "U1CCSGuW_LbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Corpus data set\n",
        "For google colab the training set it too large to use using the free teir."
      ],
      "metadata": {
        "id": "Us_nZU079MhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using full dataset training data\n",
        "! node javascript/extractFromJS.js calls --parallel 4 data/js/programs_training.txt data/js/programs_all\n",
        "! mkdir training/\n",
        "! mv calls_* training/"
      ],
      "metadata": {
        "id": "LTsJHWuOGlF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using full dataset evaluation data\n",
        "! node javascript/extractFromJS.js calls --parallel 4 data/js/programs_eval.txt data/js/programs_all\n",
        "! mkdir eval/\n",
        "! mv calls_* eval/"
      ],
      "metadata": {
        "id": "-1k5hAahb3Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subset of dataset\n",
        "50 files to split between training and evaluation"
      ],
      "metadata": {
        "id": "_QAU4w0I9rhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract training data\n",
        "! node javascript/extractFromJS.js calls --parallel 4 data/js/programs_50_training.txt data/js/programs_50\n",
        "! mkdir training/\n",
        "! mv calls_* training/"
      ],
      "metadata": {
        "id": "CCdB2wKKI1iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract Eval data\n",
        "! node javascript/extractFromJS.js calls --parallel 4 data/js/programs_50_eval.txt data/js/programs_50\n",
        "! mkdir eval/\n",
        "! mv calls_* eval/"
      ],
      "metadata": {
        "id": "GZKo1EjcYJZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Train a classifier to identify bugs\n",
        "1. Train and validate the classifier\n",
        "\n",
        "`python3 python/BugLearnAndValidate.py --pattern SwappedArgs --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --training_data calls_xx*.json --validation_data calls_yy*.json`\n",
        "\n",
        "  * The first argument selects the bug pattern.\n",
        "  * The next three arguments are vector representations for tokens (here: identifiers and literals), for types, and for AST node types. These files are provided in the repository.\n",
        "  * The remaining arguments are two lists of .json files. They contain the training and validation data extracted in Step 1.\n",
        "  * After learning the bug detector, the command measures accurracy and recall w.r.t. seeded bugs and writes a list of potential bugs in the unmodified validation code (see `poss_anomalies.txt`).\n",
        "\n",
        "2. Train a classifier for later use\n",
        "\n",
        "`python3 python/BugLearn.py --pattern SwappedArgs --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --training_data calls_xx*.json`\n",
        "\n",
        "  * Optionally, pass --out some/dir to set the output directory for the trained model."
      ],
      "metadata": {
        "id": "tgL2XKlZ98sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and validate the classifier\n",
        "! python3 python/BugLearnAndValidate.py --pattern SwappedArgs --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --training_data training/calls_*.json --validation_data eval/calls_*.json"
      ],
      "metadata": {
        "id": "_g36c4KaRGDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a classifier for later use\n",
        "! python3 python/BugLearn.py --pattern SwappedArgs --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --training_data training/calls_*.json --out bug_detection_model/"
      ],
      "metadata": {
        "id": "Wy7GsbwTDKth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Finding Bugs\n",
        "Finding bugs in one or more source files consists of these two steps:\n",
        "1. Extract code pieces\n",
        "2. Use a trained classifier to identify bugs"
      ],
      "metadata": {
        "id": "BR5Elgm338wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract code peices from file directory\n",
        "#! node javascript/extractFromJS.js calls --files <list of files>\n",
        "! node javascript/extractFromJS.js calls --files data/js/programs_50/*.js\n",
        "! mkdir find_bugs/\n",
        "! mv calls_* find_bugs/"
      ],
      "metadata": {
        "id": "5fQHa-gR4BOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a trained classifier to identify bugs\n",
        "\n",
        "`python3 python/BugFind.py --pattern SwappedArgs --threshold 0.95 --model some/dir --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --testing_data calls_xx*.json`\n",
        "\n",
        "  * The first argument selects the bug pattern.\n",
        "  * 0.95 is the threshold for reporting bugs; higher means fewer warnings of higher certainty.\n",
        "  * --model sets the directory to load a trained model from.\n",
        "  * The next three arguments are vector representations for tokens (here: identifiers and literals), for types, and for AST node types. These files are provided in the repository.\n",
        "  * The remaining argument is a list of .json files. They contain the data extracted in Step 1.\n",
        "  * The command examines every code piece and writes a list of potential bugs with its probability of being incorrect"
      ],
      "metadata": {
        "id": "Q_nA9HTuHUYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a trained classifier to identify bugs\n",
        "! python3 python/BugFind.py --pattern SwappedArgs --threshold 0.95 --model bug_detection_model/ --token_emb token_to_vector.json --type_emb type_to_vector.json --node_emb node_type_to_vector.json --testing_data find_bugs/calls_*.json"
      ],
      "metadata": {
        "id": "W0OQTBGt5NVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Embeddings for Identifiers\n",
        "\n",
        "The above bug detector rely on a vector representation for identifier names and literals. To use our framework, the easiest is to use the shipped `token_to_vector.json` file. Alternatively, you can learn the embeddings via Word2Vec as follows:\n",
        "\n",
        "1. Extract identifiers and tokens:\n",
        "\n",
        "`node javascript/extractFromJS.js tokens --parallel 4 data/js/programs_50_training.txt data/js/programs_50`\n",
        "\n",
        "  * The command produces `tokens_*.json` files.\n",
        "  \n",
        "2. Encode identifiers and literals with context into arrays of numbers (for faster reading during learning):\n",
        "  \n",
        "  `python3 python/TokensToTopTokens.py tokens_*.json`\n",
        "  \n",
        "  * The arguments are the just created files.\n",
        "  * The command produces `encoded_tokens_*.json` files and a file `token_to_number_*.json` that assigns a number to each identifier and literal.\n",
        "\n",
        "3. Learn embeddings for identifiers and literals:\n",
        "  \n",
        "  `python3 python/EmbeddingLearnerWord2Vec.py token_to_number_*.json encoded_tokens_*.json`\n",
        "\n",
        "  * The arguments are the just created files.\n",
        "  * The command produces a file `token_to_vector_*.json`."
      ],
      "metadata": {
        "id": "qNokH3eCH_Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract identifiers and tokens\n",
        "!node javascript/extractFromJS.js tokens --parallel 4 data/js/programs_50_training.txt data/js/programs_50"
      ],
      "metadata": {
        "id": "QNXyFBajIRXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode identifiers and literals with context into arrays of numbers\n",
        "! python3 python/TokensToTopTokens.py tokens_*.json"
      ],
      "metadata": {
        "id": "_OgPQFPOJjq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn embeddings for identifiers and literals\n",
        "! python3 python/EmbeddingLearnerWord2Vec.py token_to_number_*.json encoded_tokens_*.json"
      ],
      "metadata": {
        "id": "Q8EYgiy7J1ci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}